{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uhp2jERgBIQ"
      },
      "outputs": [],
      "source": [
        "# only run if encounter errors in base code\n",
        "# !pip uninstall tensorflow\n",
        "# !pip uninstall keras\n",
        "# !pip install -U tensorflow\n",
        "# !pip install -U keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "tOMji1ixcda1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# first we will import the dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize to help: improve convergence, reduce sensitivity to learning rate,\n",
        "# help effective weight sharing (different regions of images),\n",
        "# avoid vanishing/exploding gradients, and help reduce impact of variations\n",
        "# in pixel intensity and illumination\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Now utilize One-hot encoding in order to perform proper pre processing\n",
        "# in the correct numerical form for the neural net i.e one -> 1\n",
        "# Note: y set is 10 categories zero->nine\n",
        "# ensures that all output categories exist uncomment print if need example\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# print(y_train, y_test)\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
        "# print(y_train[0], y_test[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_G0hKt9rBvX"
      },
      "outputs": [],
      "source": [
        "# If your data doesn't have testing then lets make it!\n",
        "# now we should build the training and testing sets\n",
        "# we will take 80% of data for training and 20% of data to test\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "go1KcxmCgx2f"
      },
      "outputs": [],
      "source": [
        "# Lets build a CNN model!\n",
        "model = tf.keras.Sequential() # all of your layers can be a list here\n",
        "\n",
        "# simplify layers import\n",
        "import tensorflow.keras.layers as layers\n",
        "\n",
        "# Lets start with a convolutional input layer\n",
        "model.add(layers.Conv2D(\n",
        "    # how many filters should be used?\n",
        "    # These filter values start random and are learned per iteration\n",
        "    filters=32,\n",
        "    # How will the filters be applied to the image?\n",
        "    # this makes the filter slide over a 3x3 region\n",
        "    kernel_size=(3, 3),\n",
        "    # The Rectified Linear Unit (ReLU) activation function is commonly\n",
        "    # used in CNNs. It introduces non-linearity by setting all negative\n",
        "    # values to zero and leaving positive values unchanged.\n",
        "    activation='relu',\n",
        "    # shape is the dimensions of image and color channels\n",
        "    input_shape=x_train.shape[1:] + (1,)\n",
        "))\n",
        "\n",
        "# Now our convolutional layer will have given us larger\n",
        "# spatial dimensions of feature maps\n",
        "# so lets use a method called MapPooling\n",
        "# MaxPooling is a down-sampling technique used to reduce the spatial dimensions\n",
        "# of feature maps produced by convolutional layers\n",
        "# This takes some window and only keeps the maximum value within the window\n",
        "# default (2, 2)\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# How stacking covolutional layers can allow the model to define increasingly\n",
        "# complex and abstract features from input data\n",
        "model.add(layers.Conv2D(\n",
        "    filters=64, # Lets try and learn some complex features!\n",
        "    kernel_size=(3,3),\n",
        "    activation='relu'\n",
        "))\n",
        "\n",
        "# Down-sample\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Now before we pass into a fully connected (Dense) layer\n",
        "# we should flatten into a one-dimensional array\n",
        "# basically formatting for Dense\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Now that we have learned some features\n",
        "# Lets learn how these features are significant\n",
        "\n",
        "# What is a Dense Layer\n",
        "# A Dense layer represents a fully connected layer from the previous layer\n",
        "# This layer performs a **weighted sum** of inputs, adds a **bias term**, and then\n",
        "# applies an **activation function** to produce an output. The output is a non-linear\n",
        "# transformation of its input data\n",
        "# This layer is basically a information processing layer\n",
        "model.add(layers.Dense(\n",
        "    # lets define how many neurons this layer has\n",
        "    units=128,\n",
        "    activation='relu'\n",
        "))\n",
        "\n",
        "# Now lets create the output layer\n",
        "model.add(layers.Dense(\n",
        "    # there are 10 classes of output zero->nine\n",
        "    units=10,\n",
        "    activation='softmax'\n",
        "))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    # The optimizer updates the model's weights during training.\n",
        "    # Adam is an extended SGD (Stochastic Gradient Descent)\n",
        "    optimizer='adam',\n",
        "    # The loss/objective/cost function measures how well the model is\n",
        "    # performing. Quantifies the difference between predicted vs actual.\n",
        "    # categorical_crossentropy is for multiclass classification problems\n",
        "    # categorical cross-entropy loss encourages the model to assign high\n",
        "    # probabilities to the correct classes while penalizing incorrect\n",
        "    # class assignments. This makes it a suitable choice for training\n",
        "    # classification models.\n",
        "    loss='categorical_crossentropy',\n",
        "    # There are many metrics, we care about accuracy\n",
        "    # read about the different ones we can use at tensorflow\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "r5z5BQ2mr44R"
      },
      "outputs": [],
      "source": [
        "# Now we have to reshape the data to fit that extra dimension from earlier\n",
        "\n",
        "# Remember we added the 1 to tell we are using greyscale 0->255 not color images\n",
        "# print(x_train.shape)\n",
        "x_train = x_train.reshape(x_train.shape + (1,))\n",
        "# print(x_train.shape)\n",
        "x_test = x_test.reshape(x_test.shape + (1,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIa83W7js5sd",
        "outputId": "c204b55e-25cf-4b55-94fd-bfe1003f6794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "938/938 [==============================] - 65s 67ms/step - loss: 0.1538 - accuracy: 0.9548 - val_loss: 0.0464 - val_accuracy: 0.9844\n",
            "Epoch 2/5\n",
            "938/938 [==============================] - 57s 61ms/step - loss: 0.0455 - accuracy: 0.9858 - val_loss: 0.0324 - val_accuracy: 0.9894\n",
            "Epoch 3/5\n",
            "938/938 [==============================] - 57s 60ms/step - loss: 0.0309 - accuracy: 0.9901 - val_loss: 0.0399 - val_accuracy: 0.9863\n",
            "Epoch 4/5\n",
            "938/938 [==============================] - 58s 61ms/step - loss: 0.0239 - accuracy: 0.9921 - val_loss: 0.0277 - val_accuracy: 0.9899\n",
            "Epoch 5/5\n",
            "938/938 [==============================] - 57s 60ms/step - loss: 0.0167 - accuracy: 0.9943 - val_loss: 0.0303 - val_accuracy: 0.9903\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f134bd7ebf0>"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model!\n",
        "model.fit(\n",
        "    # input data\n",
        "    x=x_train,\n",
        "    # expected output\n",
        "    y=y_train,\n",
        "    # computational efficiency\n",
        "    batch_size=64,\n",
        "    # how many times should we fit our data?\n",
        "    epochs=5,\n",
        "    # Model is not trained on this\n",
        "    # Serves as a way to evaluate the model\n",
        "    # on unseen data it helps YOU understand\n",
        "    # how well the model is generalizing\n",
        "    validation_data=(x_test, y_test)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2VjP02XteTo",
        "outputId": "4d56fccd-a68a-4a37-9c7e-b6e16da1c81b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0303 - accuracy: 0.9903\n",
            "Test accuracy: 99.03%\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "# hover over evaluate to read docs\n",
        "test_loss, test_accuracy = model.evaluate(\n",
        "    x=x_test,\n",
        "    y=y_test,\n",
        "    verbose=1)\n",
        "print('Test accuracy: {:.2f}%'.format(test_accuracy * 100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
